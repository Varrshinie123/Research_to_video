{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip -q install PyPDF2==3.0.1 transformers==4.41.2 datasets==2.19.0 torch torchvision torchaudio --upgrade --quiet\n",
        "!pip -q install networkx==3.3 scikit-learn==1.5.1 sentencepiece==0.2.0 tabulate==0.9.0 --quiet\n",
        "!pip -q install TTS==0.22.0 --quiet\n",
        "!pip -q install moviepy==1.0.3 --quiet\n",
        "print('✅ Installed')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9bATbftA3We",
        "outputId": "40b53e57-37f8-4c7c-8be6-44ba35c16653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m955.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m60.3/63.6 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re, os, math, time, uuid\n",
        "from typing import List, Dict\n",
        "from pathlib import Path\n",
        "from PyPDF2 import PdfReader\n",
        "import networkx as nx\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import pipeline\n",
        "from moviepy.editor import TextClip, AudioFileClip, concatenate_videoclips\n",
        "from IPython.display import Audio\n",
        "print('✅ Imported')\n"
      ],
      "metadata": {
        "id": "3lPa-mdbA341"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # choose your research paper PDF\n",
        "PDF_PATH = list(uploaded.keys())[0]\n",
        "print('Using:', PDF_PATH)\n"
      ],
      "metadata": {
        "id": "4VCIOfMqA53z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SECTION_PATTERNS = [\n",
        "    r'^\\s*abstract\\s*$', r'^\\s*introduction\\s*$', r'^\\s*related work\\s*$',\n",
        "    r'^\\s*methods?\\s*$', r'^\\s*materials and methods\\s*$', r'^\\s*experiments?\\s*$',\n",
        "    r'^\\s*results?\\s*$', r'^\\s*discussion\\s*$', r'^\\s*conclusion\\s*$', r'^\\s*references?\\s*$'\n",
        "]\n",
        "SECTION_RE = re.compile(\"|\".join(SECTION_PATTERNS), flags=re.IGNORECASE)\n",
        "\n",
        "def read_pdf_text(pdf_path: str) -> str:\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text_chunks = []\n",
        "    for page in reader.pages:\n",
        "        txt = page.extract_text() or \"\"\n",
        "        text_chunks.append(txt)\n",
        "    return \"\\n\".join(text_chunks)\n",
        "\n",
        "def normalize(text: str) -> str:\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def split_into_paragraphs(text: str, min_len: int = 40):\n",
        "    candidates = re.split(r'\\n{2,}|(?<=\\.)\\s+', text)\n",
        "    return [normalize(p) for p in candidates if len(normalize(p)) >= min_len]\n",
        "\n",
        "def label_section_headers(paragraphs):\n",
        "    labeled, current = [], \"unknown\"\n",
        "    for p in paragraphs:\n",
        "        if SECTION_RE.match(p.lower()):\n",
        "            current = p.strip().lower()\n",
        "            labeled.append({\"text\": p, \"section_guess\": \"header\"})\n",
        "        else:\n",
        "            labeled.append({\"text\": p, \"section_guess\": current})\n",
        "    return labeled\n",
        "\n",
        "raw_text = read_pdf_text(PDF_PATH)\n",
        "paragraphs = split_into_paragraphs(raw_text)\n",
        "labeled = label_section_headers(paragraphs)\n",
        "print('Paragraphs:', len(paragraphs))\n",
        "labeled[:3]\n"
      ],
      "metadata": {
        "id": "-06T6G8AA8qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "zero_shot = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "CANDIDATE_LABELS = [\"abstract\",\"introduction\",\"methods\",\"results\",\"discussion\",\"conclusion\",\"other\"]\n",
        "\n",
        "def tag_paragraphs_zero_shot(paragraphs):\n",
        "    tagged = []\n",
        "    for p in paragraphs:\n",
        "        res = zero_shot(p, CANDIDATE_LABELS, multi_label=False)\n",
        "        tagged.append({\"text\": p, \"section\": res[\"labels\"][0], \"score\": float(res[\"scores\"][0])})\n",
        "    return tagged\n",
        "\n",
        "\n",
        "tagged = tag_paragraphs_zero_shot(paragraphs[:120])\n",
        "tagged[:5]\n"
      ],
      "metadata": {
        "id": "ds2gKaRXA-Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_sentences(text: str):\n",
        "    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n",
        "\n",
        "def textrank_summarize(text: str, top_k: int = 5):\n",
        "    sentences = split_sentences(text)\n",
        "    if len(sentences) <= top_k:\n",
        "        return \" \".join(sentences)\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words=\"english\").fit(sentences)\n",
        "    X = vectorizer.transform(sentences)\n",
        "    sim = (X * X.T).toarray()\n",
        "    for i in range(sim.shape[0]):\n",
        "        sim[i, i] = 0.0\n",
        "    nx_graph = nx.from_numpy_array(sim)\n",
        "    scores = nx.pagerank(nx_graph, max_iter=100, tol=1e-6)\n",
        "    ranked = sorted(((scores[i], s, i) for i, s in enumerate(sentences)), reverse=True)\n",
        "    chosen = sorted(ranked[:top_k], key=lambda x: x[2])\n",
        "    return \" \".join([c[1] for c in chosen])\n",
        "\n",
        "def concat_section(tagged, target):\n",
        "    return \" \".join([t[\"text\"] for t in tagged if t.get(\"section\") == target])\n",
        "\n",
        "abstract_txt = concat_section(tagged, \"abstract\")\n",
        "methods_txt  = concat_section(tagged, \"methods\")\n",
        "results_txt  = concat_section(tagged, \"results\")\n",
        "\n",
        "narration_parts = []\n",
        "if abstract_txt: narration_parts.append(\"Abstract: \" + textrank_summarize(abstract_txt, top_k=2))\n",
        "if methods_txt:  narration_parts.append(\"Methods: \" + textrank_summarize(methods_txt, top_k=3))\n",
        "if results_txt:  narration_parts.append(\"Results: \" + textrank_summarize(results_txt, top_k=3))\n",
        "\n",
        "narration_text = \" \".join(narration_parts) if narration_parts else \"Summary not available.\"\n",
        "narration_text\n"
      ],
      "metadata": {
        "id": "R1LshMcsBAm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from TTS.api import TTS\n",
        "MODEL_NAME = \"tts_models/en/ljspeech/tacotron2-DDC\"\n",
        "OUT_WAV = \"narration.wav\"\n",
        "\n",
        "tts = TTS(model_name=MODEL_NAME, progress_bar=True, gpu=True)\n",
        "tts.tts_to_file(text=narration_text, file_path=OUT_WAV)\n",
        "Audio(OUT_WAV)\n"
      ],
      "metadata": {
        "id": "y6N2AClxBDls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def text_slide(text, duration=5, width=1280, height=720):\n",
        "    return (TextClip(text, fontsize=48, size=(width, height), method=\"caption\")\n",
        "            .set_duration(duration).set_position(\"center\"))\n",
        "\n",
        "slides_text = [\"Paper Summary\", \"Methods Overview\", \"Key Findings\"]\n",
        "clips = [text_slide(t, duration=max(4, min(10, len(narration_text)//50))) for t in slides_text]\n",
        "video = concatenate_videoclips(clips, method=\"compose\")\n",
        "\n",
        "audio = AudioFileClip(OUT_WAV)\n",
        "final = video.set_audio(audio).set_duration(max(video.duration, audio.duration))\n",
        "final.write_videofile(\"paper_summary.mp4\", fps=24)\n",
        "print('Saved: paper_summary.mp4')\n"
      ],
      "metadata": {
        "id": "K7QyldZ4BHEh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}